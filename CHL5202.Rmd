---
title: 'CHL5202: Biostatistics II'
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
---


# Course Description and Objectives {.unlisted .unnumbered}
* The goal of this course is to continue development and application of skills in statistical analysis required in epidemiologic and health care research.
* Analysis where outcome is continuous and exposure and covariates are a mix of continuous and categorical variables (multiple linear regression)
* Relaxing linearity assumptions of continuous exposure variables through the use of regression splines
* Perform internal validation by means of bootstrapping or cross-validation
* Extension to binary outcomes (multiple logistic regression)
* Extension to time to event and censored outcomes (survival analysis)
* Dealing with missing data
* Introduce Bayesian methods

Course material provided by Professor Kevin Thorpe and modifications were made by myself.

# Tutorial 1

## Question 1
Load the data frame `tut1` into R from the file `tut1.RData` available on the course page. The data
frame contains the variables x and y.

Fit a simple regression with y as the response and x as the predictor. Use diagnostic plots to determine if a non-linear relationship should be modeled. If you believe a non-linear relationship is present, model it using a restricted cubic spline. The number of knots is up to you.

```{r, message=F}
# Load packages and data
library(rms)

load("./CHL5202/tut1.RData")

dd <- datadist(tut1)
options(datadist="dd")
```

* Some notes taken from [here](https://thomaselove.github.io/432-notes/using-ols-from-the-rms-package-to-fit-linear-models.html).

* Comparing `ols()` vs `lm()`: one advantage for ols is that the entire variance-covariance matrix is saved

```{r}
fit0 <- ols(y~x,data=tut1)
fit0
```

* When applying anova to an ols model, it separates out the linear and non-linear components of restricted cubic splines and polynomial terms (and product terms if they are present). 
* Unlike for `lm()`, using `anova` with `ols()` we have partial F-tests that describe the marginal impact of removing each covariate from the model.

```{r}
anova(fit0)
```

* Next, we check the model assumptions using diagnostic plots. 
* In the Normal Q-Q Plot we are looking for normally distributed error terms, i.e. most of the points lie on the line and do not show a non-linear pattern. 

```{r}
# for diagnostic plots
dx0 <- data.frame(tut1, e = resid(fit0), yhat = fitted(fit0)) 

# Normal QQ Plot
qqnorm(dx0$e)
qqline(dx0$e)
```

* `xyplot()`: Common Bivariate Trellis Plots. Produces conditional scatterplots, typically between two continuous variables. Below we have residual plots. 
* We are looking for the residuals to not show a distinct (non-linear) pattern which would suggest that a transformation is required.

```{r}
# Residual Plots
xyplot(e~x, data = dx0, type = c("p","smooth"))
xyplot(e~yhat, data = dx0, type = c("p","smooth"))
```

* A non-linear relationship is present, so we will model it using a restricted cubic spline. For this example, we are going to use 4 knots.

```{r}
fit1 <- ols(y ~ rcs(x, 4), data = tut1)

fit1
```

* It is difficult to interpret the coefficients above. The `anova()` output below is more useful and shows a significant non-linear component.

```{r}
anova(fit1)
```


```{r}
# Plot predicted fit with confidence interval
ggplot(Predict(fit1))
```


```{r}
# for diagnostic plots
dx1 <- data.frame(tut1, e = resid(fit1), yhat = fitted(fit1))

# Normal QQ Plot
qqnorm(dx1$e)
qqline(dx1$e)
```


```{r}
# Residual plots
xyplot(e ~ x, data = dx1, type = c("p", "smooth"))
xyplot(e ~ yhat, data = dx1, type = c("p", "smooth"))
```

* There is no longer a non-linear pattern in the residuals.


## Question 2
Load the data frame FEV from the file FEV.RData. For these data, use the variable fev as the response and the rest as the explanatory covariates.

Fit an additive linear model to fev using the other variables as the covariates. Evaluate whether any of the continuous variables should be fit as non-linear terms. 

Fit an another additive model but this time model the continuous covariates with restricted cubic splines with four knots. Describe the results.

```{r}
load("./CHL5202/FEV.RData")

fev.dd <- datadist(FEV)
options(datadist = "fev.dd")
```


```{r, message=F}
# `dplyr()`: package for data wrangling - part of `tidyverse()`
library(dplyr)

# Useful for quickly viewing the data
glimpse(FEV)

summary(FEV)
```


```{r}
# Fit initial additive model
fev.lin <- ols(fev ~ age + height + sex + smoke, data = FEV)
fev.lin
```

```{r}
anova(fev.lin)
```

* `age` and `height` are continuous variables, so we can use residual plots.
* `sex` and `smoke` are categorical variables (in fact, they're binary) so residual plots won't be useful.

```{r}
dx.fev <- data.frame(FEV, e = resid(fev.lin), yhat = fitted(fev.lin))

xyplot(e ~ age, data = dx.fev, type = c("p", "smooth"))
xyplot(e ~ height, data = dx.fev, type = c("p", "smooth"))
```

* The residual plots show that non-linearity needs to be modelled for both variables.
* Once again, we shall use restricted cubic splines with 4 knots.

```{r}
fev.full <- ols(fev ~ rcs(age, 4) + rcs(height, 4) + sex + smoke, data = FEV)

fev.full
```

```{r}
anova(fev.full)
```

* The `anova()` output suggests that the non-linear components for age and height are statistically significant.

```{r}
dx.full <- data.frame(FEV, e = resid(fev.full), yhat = fitted(fev.full))

# Residual plots look good
xyplot(e ~ age, data = dx.full, type = c("p", "smooth"))
xyplot(e ~ height, data = dx.full, type = c("p", "smooth"))
```

* Running `summary()` is the best way to get the effects of the categorical variables.
* For continuous variables, it is better to plot.

```{r}
summary(fev.full)
```

```{r}
ggplot(Predict(fev.full, age))
ggplot(Predict(fev.full, height))
```


## Extra
* `%>%`: pipe operator - very useful for performing multiple data wrangling steps
* `ggplot2()`: plotting package, included in `rms()` and `tidyverse()`

```{r}
# Boxplot
FEV %>%
  ggplot(mapping = aes(x = sex, y = height, fill = sex)) +
  geom_boxplot() +
  theme_bw() +
  labs(title = "Boxplot of heights by sex")
```


# Tutorial 2

## Question 1
Use the hwy2.RData file available from the course page. After attaching the rms package and doing the usual `datadist()/options()` task, fit the following model:

`fit <- ols(rate~rcs(trks,4)+rcs(sigs1,4)+type+hwy,data=hwy2,x=TRUE,y=TRUE)`

Run both the ordinary bootstrap validation and the .632 bootstrap validation on this model. Use the same number of iterations and seed for both validations (to compare the results). What are your conclusions?

```{r, message=F}
library(rms)

load("./CHL5202/hwy2.RData")

h.dd <- datadist(hwy2)
options(datadist="h.dd")

fit <- ols(rate ~ rcs(trks,4) + rcs(sigs1,4) + type + hwy, data=hwy2, x=TRUE, y=TRUE)
```


```{r}
# Standard bootstrap
set.seed(321) # Set seed is reproducibility. Choose any number you like.
validate(fit, B=100)

# .632 bootstrap
set.seed(321)
validate(fit, method = ".632", B=100)
```

Conclusions:

* There is a discrepancy in standard bootstrap vs .632 method -- the standard bootstrap may be overfitting so we use the .632 method to double check the validation results. The .632 bootstrap applies a correction factor to reduce the bias.

* Standard bootstrap may believe the model is less overfit than what it actually is, i.e. low optimism so `index.corrected` is closer to `index.orig`.

* The drop in R-square is very large from validation (went from positive to negative), indicating the original model was very overfit to the point where the relationship inverted.

* Slope change seems to imply an adjusted slope estimate should be around 0.75 of what the original model guessed.


## Extra
Let's investigate the outputs from a simpler model.
```{r}
# We remove the restricted cubic splines which may be the main source of overfitting
fit2 <- ols(rate ~ trks + sigs1 + type + hwy, data=hwy2, x=TRUE, y=TRUE)

set.seed(321)
validate(fit2, B=100)
set.seed(321)
validate(fit2, method = ".632", B=100)
```

* For the .632 bootstrap, the magnitude of the optimism for all 5 values is smaller than in the saturated model which showed overfitting.

* Hard to say whether this model is still overfitting but it is certainly an improvement.

* The R-square term in particular shows a major improvement, as the test R-square is positive rather than negative like previously shown.


# Tutorial 3

## Question 1
Load the data from `tutdata` from the file `tutdata.RData` posted on the course page. Fit an additive model where `y` is the outcome and the predictors are `blood.pressure`, `sex`, `age`, and `cholesterol` but they should use a restricted cubic spline with four knots for cholesterol.

```{r, message=F}
library(rms)
load("./CHL5202/tutdata.RData")

dd <- datadist(tutdata)
options(datadist="dd")

fit <- lrm(y ~ blood.pressure + sex + age + rcs(cholesterol,4), data = tutdata)
```


## Question 2
Which "variables" are significant at the 5% level?

```{r}
anova(fit)
```

* At the 5% significance level, `sex` and `age` are statistically significant with p-values of 0.0002 and < 0.0001 respectively.


## Question 3
Display the cholesterol effect.

```{r}
# log odds
plot(Predict(fit,cholesterol))

# probability
plot(Predict(fit,cholesterol,fun=plogis), ylab="Probability")

# odds
plot(Predict(fit,cholesterol,fun=exp), ylab="Odds")
```


## Question 4
Describe the effect of sex.

```{r}
summary(fit)
```

* The odds of the outcome `y` for males is 1.61 times the odds for females (with 95% CI [1.25,2.08]) controlled for blood pressure, age and cholesterol.


## Extra
* Interpreting odds ratios for continuous variables: for every one unit increase in the continuous variable, the odds of the outcome multiply by $\exp(\beta)$, where $\beta$ represents the log-odds ratio.
* E.g. referring to Question 4 output: for every one year increase in age, the odds of the outcome multiply by 1.55. Alternatively, it might be more intuitive to say that the odds of the outcome increases by 55%.


# Tutorial 4

## Question 1
Use the data from last week (`tutdata.RData`) and re-fit the model from last week which was an additive model where `y` is the outcome and the predictors are `blood.pressure`, `sex`, `age`, and `cholesterol` except this time use a restricted cubic spline with four knots for `cholesterol`. Re-run the anova also.

```{r, message=F}
library(rms)
load("./CHL5202/tutdata.RData")

dd <- datadist(tutdata)
options(datadist="dd")

fit1 <- lrm(y ~ blood.pressure + sex + age + rcs(cholesterol,4), data = tutdata)
anova(fit1)
```


## Question 2
Fit a model as above only this time, add interactions between sex and age as well as sex and the RCS for cholesterol and run anova() on this model.

```{r}
fit2 <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)), data = tutdata)
anova(fit2)
```

* All interactions with sex are statistically significant.

## Question 3
Compare the two models with a likelihood ratio test. What does it show?

```{r}
lrtest(fit1,fit2)
```

* We are testing the null hypothesis that all of the coefficients for the interaction terms are equal to 0. 
* Degrees of freedom (df): 10 df (more complicated model) minus 6 df (simpler model) equals 4 df.
* With the extremely small p-value from the likelihood ratio test, we reject the null hypothesis and conclude that at least one of the coefficients is not equal to 0.
* This p-value is equivalent to the p-value for `TOTAL INTERACTION` in the `anova(fit2)` output as this is what we are testing.


## Question 4
Describe the sex/age and sex/cholesterol relationships with outcome.

```{r}
plot(Predict(fit2,age,sex))
plot(Predict(fit2,cholesterol,sex))
```

* We can use the non-overlapping confidence intervals to specify statistical significance. If this is too complicated we can also discuss the points where the lines cross. This is an open ended question.
* At ages below 50, the log odds of the outcome is statistically significantly higher for males compared to females. From ages 50 to 70, the log odds is still higher for males but the confidence intervals overlap so the difference is not as significant.
* For cholesterol levels below 150, the log odds of the outcome is statistically significantly higher for females compared to males. The opposite is true in cholesterol levels from 190 to 200, and above 220. There is a bit of overlap in confidence intervals between the 200-220 range.

```{r}
# Fancy way of combining the plots together but is harder to interpret
bplot(Predict(fit2,cholesterol,age,sex,np=20),lfun=wireframe,drape=TRUE)
```

## Extra

* Below we are fitting a third model with an interaction term between sex and the other 3 variables, including blood pressure.
```{r}
fit3 <- lrm(y ~ sex * (blood.pressure + age + rcs(cholesterol,4)), data = tutdata)
# Alternate way
#lrm(y ~ sex + blood.pressure + age + rcs(cholesterol,4) + sex:blood.pressure + sex:age + sex:rcs(cholesterol,4), data = tutdata)

anova(fit3)
```

* The total interaction from the anova output is still statistically significant as it takes into account all interactions. 
* We can also test for it below as follows:

```{r}
lrtest(fit1,fit3)
```

* The sex and blood pressure interaction term from the anova output is not significant (p=0.494). 
* This is equivalent to the p-value from the likelihood ratio test below:

```{r}
lrtest(fit2,fit3)
```

* Let's try to plot the sex/blood pressure relationship:

```{r}
plot(Predict(fit3,blood.pressure,sex))
```

* At a first glance we do not see the lines meet like they do in the sex/age and sex/cholesterol interactions. The confidence intervals also overlap for the most part, except in the 115-130 range. 
* Personally, I would avoid using this as a standalone method to check for a significant interaction, you should use either the anova function call or the likelihood ratio test.


# Tutorial 5

## Question 1
Using the data from the last few tutorials, fit the following model.

`fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)), data = tutdata, x=TRUE, y=TRUE)`

```{r, message=F}
library(rms)
load("./CHL5202/tutdata.RData")

dd <- datadist(tutdata)
options(datadist = "dd")
```


```{r}
fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol, 4)), data = tutdata, 
           x = TRUE, y = TRUE)
fit
```


## Question 2
Validate the model using the bootstrap with 100 replications and comment on the overfitting.

```{r}
set.seed(123)
validate(fit, B = 100)
```

* There is mild evidence of overfitting.
* $D_{xy}$ and Brier score (`B`) do not change much at all.
* Slope parameter indicates a need to shrink by about 12% to calibrate


## Question 3
Investigate influential observations and partial residual plots.

```{r}
dff <- resid(fit, "dffits")
plot(dff)
```

* The DFFITS statistic indicate the effect that deleting each observation has on the predicted values of the model.
* Some use $|\text{DFFITS}| > 2\sqrt{\frac{p+1}{n-p-1}}$ to indicate an influential observation.

```{r}
show.influence(which.influence(fit), data.frame(tutdata, dffits = dff), report = c("dffits"))
```

* Using `which.influence()` gives the indices of influential observations, which are listed in the first column of the dataframe above.
* The variables with significant DFBETA are marked with an asterisk. Note that all of these observations have significant DFFITS.
* The DFBETAS are statistics that indicate the effect that deleting each observation has on the estimates for the regression coefficients.
* For DFBETAS one can use $|\text{DFBETAS}| > u$ for a given cutoff, the default is 0.2.

Below we investigate partial residual plots which can be used to verify the functional form of continuous covariates. 
```{r}
# Option below saves the following plots to a pdf file
# pdf("resid_plots.pdf")

resid(fit, "partial", pl = "loess")
```

* There is very little evidence of non-linearity in most of these plots, i.e. there is not much value in including a non-linear component for these variables.


# Tutorial 6

Over the next few weeks the tutorials will use the data frame `mgus2` which may be found in the survival package. Since the survival package is attached when `rms` is, nothing special is required to access it, other than loading the `rms` package.

```{r, message=F}
library(rms)

str(mgus2)
```

More information about the data can be found in the help file (e.g. `?mgus2`).


## Question 1
Estimate the Kaplan-Meier curves for survival for males and females and plot the result.

```{r}
# The rms package has a function npsurv which is used in place of the usual survfit.
fit <- npsurv(Surv(futime, death) ~ sex, data = mgus2)

# Two methods below:
plot(fit, lty = c(1, 2)) # standard method

survplot(fit, n.risk = TRUE) #a more flexible function from rms
```


## Question 2
Test $H_0 : S_F(t) = S_M(t)$ versus $H_0 : S_F(t) \neq S_M(t).$

```{r}
# This requires the Mantel-Haenzel (log-rank) test.
print(fit.mh <- survdiff(Surv(futime, death) ~ sex, data = mgus2))
```

* The Mantel-Haenzel test yields a p-value of 0.002, so we reject the null hypothesis as there is strong evidence that the survival function for females is not equal to the survival function for males.


## Question 3
Compute the risk difference at one year.

```{r}
(summ <- summary(fit, times = 12)) # time is in months
```

* The risk difference (assigning females as control and males as treatment) is:

$$ 
\begin{aligned}
\widehat{RD} &= \hat{F}_C(t_0)-\hat{F}_T(t_0) \\
&= (1-\hat{S}_C(t_0))(1-\hat{S}_T(t_0)) \\
&= \hat{S}_T(t_0)-\hat{S}_C(t_0) \\
&= 0.851 - 0.9032 \\
&= -0.0522
\end{aligned}
$$

* Using R:

```{r}
(rd <- summ$surv[2] - summ$surv[1])

fit$surv[length(fit$surv)-232+12] - fit$surv[12] # alternative
```

* Interpretation: The observed risk of death in females at one year is 5.2% lower than in males.

* (Extra) A 95% confidence interval for the risk difference is:

$$
\begin{aligned}
&\widehat{RD} \pm 1.96\sqrt{\hat{V}[\hat{S}_C(t_0)] + \hat{V}[\hat{S}_T(t_0)]} \\
&= -0.0522 \pm 1.96\sqrt{0.0118^2 + 0.013^2} \\
&= (-0.0866,-0.0178)
\end{aligned}
$$

* Using R:

```{r}
bound <- 1.96*sqrt(summ$std.err[1]^2 + summ$std.err[2]^2)

c(rd-bound, rd+bound)
```


## Question 4
Compute a hazard ratio using the output from `survdiff`.

$$ 
\widehat{HR} = \frac{O_T/E_T}{O_C/E_C} = \frac{540/492}{423/471} = 1.222
$$

* Alternatively, we can calculate using `fit.mh`:

```{r}
# We can extract the numbers from fit.mh 
fit.mh$obs
fit.mh$exp
```

```{r}
# Observed / Expected 
female <- fit.mh$obs[1]/fit.mh$exp[1] # 423/471
male <- fit.mh$obs[2]/fit.mh$exp[2] # 540/492

# Hazard Ratio Using females as reference
hr <- male/female # (540/492)/(423/471)
hr
```

* The hazard ratio of 1.222 means that the rate of death for males is 1.222 times the rate of death for females.

* (Extra) Obtaining a 95% CI for the hazard ratio, we start with the standard error:

$$
SE[\log\widehat{HR}] = \sqrt{\frac{1}{E_T} + \frac{1}{E_C}} = \sqrt{\frac{1}{492} + \frac{1}{471}} = 0.06446
$$

* Which gives a 95% CI of:

$$
1.222 \times e^{\pm1.96\times0.06446} = (1.0769,1.3866)
$$

* Using R:

```{r}
# Standard error of log hazard ratio
se <- sqrt((1/fit.mh$exp[2]) + (1/fit.mh$exp[1]))

# 95% Confidence interval
c(1.222*exp(-1.96*se), 1.222*exp(+1.96*se))
```

