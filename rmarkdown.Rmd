---
title: "R Projects Overview"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
---

This page will contain projects completed using R Markdown. I highly recommend using it for projects in RStudio, along with the `tidyverse`, a collection of R packages for data science. Here is a useful <a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Tidyverse+Cheat+Sheet.pdf"> tidyverse cheatsheet</a>.

## Calibrating a Snow Gauge
Snow gauges are used to indirectly measure the density of snow; a high snow density leads to less absorption of water. Analyzing this information is important because we want to monitor water levels and prevent floods from occurring. My analysis involves specifying the relationship between `density` of polyethylene blocks (a substitute for snow) and `gain` -- an amplified version of gamma photon count.

Let's load the tidyverse package and the snow gauge data.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# Load data
gauge <- readr::read_table("https://www.stat.berkeley.edu/~statlabs/data/gauge.data",col_types = "dd")
gauge <- gauge[rowSums(is.na(gauge)) != ncol(gauge),]
glimpse(gauge)
```

Let's plot the data and residuals.

```{r, message=FALSE, warning=FALSE}
# Plot density vs gain
gauge %>%
  ggplot(aes(x=gain, y=density)) + 
  theme_classic() +
  geom_point(pch=21) +
  labs(title="Density vs Gain",
       subtitle="Gauge data",
       x="Gain",
       y="Density"~(g/cm^{3}))
# Residuals
gauge_lm1 <- lm(density~gain, data=gauge)
data_frame(resid = residuals(gauge_lm1),
           fitted = fitted(gauge_lm1)) %>%
  mutate_at("resid",funs( (. - mean(.)) / sd(.))) %>%
  ggplot(aes(x = fitted,y = resid)) +
  theme_classic() +
  geom_point(pch=21) +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = -2,linetype="dashed",colour="red") +
  geom_hline(yintercept = 2,linetype="dashed",colour="red") +
  labs(title = "Residuals vs Fitted Values",
       subtitle = "Normal linear model for Gauge data",
       x = "Fitted Values",
       y = "Residuals")
```

From the *Density vs Gain* plot, it appears as though there is an inverse exponential relationship between the variables. A linear model was initially created, however the *standardized residuals* appear to follow a distinct pattern, so a standard linear model cannot directly be fit to the data. We need to transform the data.

```{r, message=FALSE, warning=FALSE}
#Box-Cox transformation
library(MASS)
gain_boxcox <- boxcox(gain ~ 1,data=gauge)
gain_boxcox$x[which(gain_boxcox$y == max(gain_boxcox$y))]
#Log transformation
gauge_transform <- gauge %>%
  mutate(log_gain = log(gain))
```

A *box-cox transformation* was done on the `gain` variable, and the plot shows that a value of $\lambda$ = 0.02020202 is the best power transformation; in this case, a log transformation is appropriate. Now let's fit a model of `density` vs `log(gain)` and make some new plots.

```{r, message=FALSE, warning=FALSE}
# Plot density vs log(gain)
gauge_transform %>%
  ggplot(aes(x=log_gain, y=density)) + 
  theme_classic() +
  geom_point(pch=21) +
  geom_smooth(method = "lm") +
  labs(title="Density vs log(Gain)",
       subtitle="Transformed log model for Gauge data",
       x="log(Gain)",
       y="Density"~(g/cm^{3}))
# Residuals
gauge_lm2 <- lm(density ~ log_gain, data=gauge_transform)
data_frame(resid = residuals(gauge_lm2),
           fitted = fitted(gauge_lm2)) %>%
  mutate_at("resid",funs( (. - mean(.)) / sd(.))) %>%
  ggplot(aes(x = fitted,y = resid)) +
  theme_classic() +
  geom_point(pch=21) +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = -2,linetype="dashed",colour="red") +
  geom_hline(yintercept = 2,linetype="dashed",colour="red") +
  labs(title = "Residuals vs Fitted Values",
       subtitle = "Transformed log model for Gauge data",
       x = "Fitted Values",
       y = "Residuals")
# Normal Q-Q
gauge_transform %>%
  mutate_at("log_gain", funs((. - mean(.)) / sd(.))) %>%
  arrange(log_gain) %>%
  mutate(q = qnorm(1:n() / (n() + 1))) %>%
  ggplot(aes(x = q,y = log_gain)) +
  theme_classic() +
  geom_point(pch=21) +
  geom_abline(slope = 1,intercept = 0,colour = "red") +
  labs(title = "Normal QQ-plot",
       subtitle="Transformed log model for Gauge data",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")
```

After completing a log transformation on the `gain` variable, a valid linear model for *Density vs log(Gain)* was produced since the new *Residuals vs Fitted Values* plot does not show a distinct pattern. Also, the *Normal QQ plot* on the transformed data does not show evidence of skew -- the normality condition is met. 

```{r, message=FALSE, warning=FALSE}
#Regression Output
summary(gauge_lm2)
```

The *regression output* shows a significant relationship between `log(Gain)` and `density`, as the p-value is extremely small. In addition, the multiple R-squared value of 0.9958 provides further evidence that this model is appropriate.

The linear model is: mean `density` = 1.298013 g/cm^3^ - (0.216203 g/cm^3^ * `log(gain)`). This model can be used to estimate the mean `density` of snow at a particular value of `gain` since the snow gauge has now been calibrated, but we must proceed with caution because polyethylene blocks were used in place of snow blocks for the model.

## Dungeness Crab Growth
As Dungeness crabs grow, they need to replace their carapace; a process referred to as molting. My analysis involves grouping the adult female Dungeness crabs by whether they recently molted or not, estimating the mean carapace size of both groups, then determining whether there is a significant difference between the groups.

Let's load the crab growth data.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# Load data
crab <- readr::read_table("https://www.stat.berkeley.edu/users/statlabs/data/crabpop.data",col_types = "dc")
glimpse(crab)
```

First let's create a *boxplot* of the shell size (`size`) by the shell type (`shell`). Shell type 0 represents a fouled carapace which can be interpreted as an old shell, while shell type 1 represents a clean carapace -- a recently molted shell. 

```{r, message=FALSE, warning=FALSE}
#Boxplot
crab %>%
  ggplot(aes(x=shell, y=size)) +
  theme_classic() +
  geom_boxplot() +
  labs(title="Boxplot of shell size by type",
       subtitle = "Crab Data",
       x = "Shell Type",
       y = "Shell Size"~(mm))
```

The boxplot shows that older shells contain some outliers, while recent shells have no outliers. Now take a look at the summary statistics:

```{r, message=FALSE, warning=FALSE}
group_means <- crab %>%
  group_by(shell) %>%
  summarize(group_mean = mean(size),
            group_median = median(size),
            group_sd = sd(size),
            group_size = n())
group_means
```

So type 0 shells are larger than type 1 shells by about 7mm on average. The next part involves statistical tests:

```{r, message=FALSE, warning=FALSE}
t_test <- t.test(size ~ shell, data = crab, var.equal = TRUE)
t_test
f_test <- var.test(size ~ shell, data = crab)
f_test
```

The *two sample t-test* yielded a statistically significant p-value; this indicates that the means of the 2 groups are not equivalent. The two sample groups are independent, since the traps that were used were designed to catch adult female Dungeness crabs of all sizes, meaning that this sample is representative of the population. An *F-test to compare two variances* shows that the two sample group variances are similar -- the constant variance condition is met.

Let's check for normality:

```{r, message=FALSE, warning=FALSE}
#Normal QQ
crab %>%
  mutate_at("size",funs( (. - mean(.)) / sd(.))) %>%
  arrange(size) %>%
  mutate(q = qnorm(1:n() / (n() + 1))) %>%
  ggplot(aes(x = q,y = size)) +
  theme_classic() +
  geom_point(pch=21) +
  geom_abline(slope = 1,intercept = 0,colour = "red") +
  labs(title = "Normal QQ-plot",
       subtitle = "Crab Data",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")
# Histogram
crab %>%
  ggplot(aes(x = size)) +
  theme_classic() +
  geom_histogram(aes(y = ..density..),colour="black",fill="lightblue",bins = 15) +
  geom_density(colour = "red") +
  labs(title = "Histogram of Shell Size",
       subtitle = "Crab Data",
       x = "Shell Size (mm)",
       y = "Density")
```

Both the *Normal QQ Plot* and *Histogram of Shell Size* show skew in the data, which may be a problem. Fortunately, the sample size of 362 (161 type 0, 201 type 1) is sufficiently large. By the Central Limit Theorem, means of samples from a population approach a normal distribution as sample size increases -- regardless of the population distribution. Thus, the normality condition for the t-test is met.

Given the strong supporting evidence, adult female Dungeness crabs with older carapaces (shell type 0) on average have larger shells than those with recently molted carapaces (shell type 1).

## Nodal Involvement in Prostate Cancer
When deciding on how to treat prostate cancer, physicians use a cancer staging system which takes into account the presence of cancer in the surrounding lymph nodes, referred to as nodal involvement. My analysis involves determining whether prostate cancer has spread to the lymph nodes based on certain characteristics.

Let's load the data from the `SMPracticals` package:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# Load SMPracticals package
# install.packages("SMPracticals")
library(SMPracticals)
data(nodal)
nodal_tbl <- as_data_frame(nodal)
dplyr::glimpse(nodal_tbl)
```

Starting with the *Nodal Involvement by Predictor* graph, it is difficult to tell whether any of the five characteristics are successful in predicting nodal involvement.

```{r, message=FALSE, warning=FALSE}
# Nodal involvement by predictor
nodal_tbl %>%
  gather(variable,value,aged:acid) %>%
  ggplot(aes(x = value,y = r)) +
  theme_classic() +
  facet_wrap(~variable) +
  geom_jitter(width=0.3,height=0.3,alpha = 0.4) +
  scale_y_continuous(breaks = c(0,1),labels = c("No Involvement","Nodal Involvement")) +
  labs(title = "Nodal Involvement, by Predictor",
       subtitle = "Nodal Data",
       x = "Predictor Value",
       y = "Nodal Involvement?")
```

Upon closer inspection, it appears as though `stage`, `acid` and `xray` have more true positive and true negative data points than false positive and false negative data points, which means that they may have a higher success rate when predicting nodal involvement.

Let's try fitting a model:

```{r, message=FALSE, warning=FALSE}
# Fit an initial binary logistic regression model
nodal_glm1 <- glm(r ~ aged + stage + grade + xray + acid,data = nodal_tbl,family = binomial)
summary(nodal_glm1)
```

An initial *binary logistic regression model* shows that `acid` and `xray` are considered somewhat significant, `stage` is close to the standard significance level of 0.05, while `age` and `grade` are not close to the significance level at all.

To explore the potentially significant predictors further, let's fit a second *binary logistic regression model*, with nodal involvement (`r`) as the response and `stage`, `acid` and `xray` as the predictors. 

```{r, message=FALSE, warning=FALSE}
# Try simpler binary logistic regression model
nodal_glm2 <- glm(r ~ stage + xray + acid,data = nodal_tbl,family = binomial)
summary(nodal_glm2)
# Analyze the deviance
anova(nodal_glm2, test="Chisq")
```

The *analysis of deviance table* for the second model shows a significant reduction in the residual deviance as each of the three variables are added to the null model.

```{r, message=FALSE, warning=FALSE}
# Transform factor type into numeric type
nodal_transform <- transform(nodal_tbl, aged = as.numeric(aged),
                                        stage = as.numeric(stage),
                                        grade = as.numeric(grade),
                                        xray = as.numeric(xray),
                                        acid = as.numeric(acid))
# Correlation matrix
round(cor(nodal_transform %>% dplyr::select(-c(m,r))),2)
# We can visualize this better using corrplot
corrplot::corrplot(cor(nodal_transform %>% dplyr::select(-c(m,r))),order="AOE")
```

In regards to the model assumptions, the values are discrete (0 or 1) and there are also no outliers in the data since the z-value for each predictor is under 3. Also, there is low intercorrelation among the predictors, as shown in the *correlation matrix*.

To clarify what each predictor represents, `stage` is a measure of the size and position of the tumour, `xray` indicates how serious the cancer is from an X-ray reading, and `acid` represents the level of acid phosphatase in the blood serum. These three variables may be helpful indicators of nodal involvement in prostate cancer, from evidence provided by the model. However, physicians should proceed with caution as there are some observations which incorrectly predict nodal involvement.

## Smoking, Age and Death
Smoking is a major health concern among the population, however many individuals of numerous age groups continue to smoke. The goal is to analyze potential relationships between age group, smoking status and mortality rate among women.

Let's load and view the data:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
#Load data from the library
library(SMPracticals)
data(smoking)
smoking_tbl <- as_data_frame(smoking)
dplyr::glimpse(smoking_tbl)
```

Now we need a way to view correlations between variables before fitting any models, and in this case a table is most suitable.

```{r, message=FALSE, warning=FALSE}
# Relationship between mortality and smoking
xtabs(cbind(dead,alive) ~ smoker,data=smoking_tbl) %>% prop.table(1)
```

Looking at potential relationships, the first table shows that a greater proportion of smokers in the study were alive after 20 years than non-smokers.

Let's try a binomial regression model:

```{r, message=FALSE, warning=FALSE}
# Binomial regression, mortality against smoking
smoking_glm1 <- glm(cbind(dead,alive) ~ smoker, data = smoking_tbl, family = binomial)
summary(smoking_glm1)
```

The *binomial regression model* for mortality against smoking shows a significant negative relationship between the variables, which indicates that smoking decreases mortality rate. This is unexpected, but another factor (age) has not been taken into account, which could explain this unusual relationship. Also, the residual deviance is quite large compared to its degrees of freedom, so this model is not a good fit.

To investigate this unintuitive relationship, we display a different table to show the relationship between smoking and age in groups of `dead` or `alive`. 

```{r, message=FALSE, warning=FALSE}
# Relationship between smoking and age in two groups: dead or alive.
xtabs(cbind(dead,alive) ~ smoker + age,data=smoking_tbl) %>% prop.table(2)
```

In this table, there is a larger proportion of younger women who smoke, relative to older women who smoke. Many of these younger women who smoke were still alive after 20 years into the study, while many of the older women passed away.


Now try another *binomial regression model*, but with age groups as a predictor:

```{r, message=FALSE, warning=FALSE}
# Binomial regression, mortality against age and smoking
smoking_glm2 <- glm(cbind(dead,alive) ~ age + smoker, data = smoking_tbl, family = binomial)
summary(smoking_glm2)
```

This model is a very strong fit since the residual deviance is quite small relative to its degrees of freedom. Now that `age` has been accounted for, the `smoker` variable is positively correlated with mortality; this is an example of Simpson's paradox. The dependence of smoking status and mortality rate are explained by their respective relationship with age (i.e. smoking and mortality are dependent, conditional on age).

If investigators in this study did not measure age, they may have incorrectly concluded that smoking correlates with a lower risk of death. In observational studies such as this one, investigators need to be careful in drawing conclusions before considering other factors that can influence relationships between the variables of interest.
